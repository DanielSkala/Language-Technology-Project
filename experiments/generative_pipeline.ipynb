{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mo\\Documents\\Organisations\\RUG\\Language-Technology-Project\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Union, Any, Optional\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), '../../Language-Technology-Project'))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    print(\"ALready in current dir\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "C:\\Users\\mo/.cache\\huggingface\\modules\\transformers_modules\\mosaicml\\mpt-1b-redpajama-200b\\a0929818043de593a6b4248481e18a4b276f5e1e\\attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "size_gb(model) = 4.89 GB\n",
      "vocab size: 50254\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained('mosaicml/mpt-1b-redpajama-200b', trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "print(f\"size_gb(model) = {model.num_parameters() * 4 / 1024**3:.2f} GB\")\n",
    "print(\"vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tokens:\n",
      "1:  doctor (0.29)\n",
      "2:  nurse (0.10)\n",
      "3:  woman (0.05)\n",
      "4:  mother (0.03)\n",
      "5:  house (0.02)\n"
     ]
    }
   ],
   "source": [
    "def predict_next_token(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    encoded_input['attention_mask'] = encoded_input['attention_mask'].bool()\n",
    "    output = model(**encoded_input)\n",
    "    next_token_logits = output.logits[:, -1, :]\n",
    "    likelihoods = torch.softmax(next_token_logits, dim=-1)\n",
    "    sorted_likelihoods, sorted_indices = torch.sort(likelihoods, descending=True)\n",
    "    return sorted_likelihoods, sorted_indices\n",
    "\n",
    "text = \"Man is a doctor as a women is a\"\n",
    "sorted_likelihoods, sorted_indices = predict_next_token(text)\n",
    "print(\"Top 5 tokens:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}: {tokenizer.decode(sorted_indices[0, i])} ({sorted_likelihoods[0, i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', 0.004680944606661797),\n",
       " ('5', 0.0044109150767326355),\n",
       " ('6', 0.004199830815196037),\n",
       " ('7', 0.004028704948723316),\n",
       " ('8', 0.0039849067106842995),\n",
       " ('9', 0.003596288850530982)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_likelihoods_of_words_given_context(prompt: str, words: list, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of likelihoods of words given a prompt.\n",
    "    \"\"\"\n",
    "    likelihoods, _ = predict_next_token(prompt)\n",
    "    word_probs = []\n",
    "    for word in words:\n",
    "        tokens = tokenizer.encode(word)\n",
    "        # word_probs.append(likelihoods[0, tokenizer.encode(word)[0]].item())\n",
    "        probability = 1\n",
    "        for token in tokens:\n",
    "            probability *= likelihoods[0, token].item()\n",
    "        word_probs.append(probability)\n",
    "    \n",
    "    output = list(zip(words, word_probs))\n",
    "    output.sort(key=lambda x: x[1], reverse=True)\n",
    "    return output\n",
    "# example\n",
    "prompt = \"2+2=\"\n",
    "options = [\"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "get_likelihoods_of_words_given_context(prompt, options, model, tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import categories\n",
    "\n",
    "class LikelihoodBasedCategoryClassifier:\n",
    "\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "\n",
    "    def __shuffle_categories(self):\n",
    "        np.random.shuffle(self.categories)\n",
    "    \n",
    "    def __construct_classification_prompt(self, sentence, max_length=128, categories=None):\n",
    "        prompt = \"Categories of Human Value Detection 2023 by name and description: \\n\\n\"\n",
    "        expectation2category = {}\n",
    "        for i, category in enumerate(categories):\n",
    "            name = category[\"name\"]\n",
    "            description = category[\"description\"]\n",
    "            examples = category[\"examples\"]\n",
    "            example = examples[np.random.randint(len(examples))]\n",
    "            example = example[:max_length] + \"...\" if len(example) > max_length else example\n",
    "            # otpions a-z\n",
    "            prompt += f\"- {name}: {description} (e.g., {example})\\n\"\n",
    "            expectation2category[name] = name\n",
    "        prompt += \"\\n\"\n",
    "        prompt += \"Classify the following sentence into one of the categories above:\\n\"\n",
    "        prompt += f\"\\n'Argument: {sentence}\"\n",
    "        prompt += \"\\Category: \"\n",
    "\n",
    "        return prompt, expectation2category\n",
    "\n",
    "    def classify_once(self, sentence: str) -> List[float]:\n",
    "        self.__shuffle_categories()\n",
    "        prompt, expectations = self.__construct_classification_prompt(sentence, categories=self.categories)\n",
    "        likelihoods = get_likelihoods_of_words_given_context(prompt, expectations.keys(), model, tokenizer)\n",
    "        likelihoods = [(expectations[c], p) for c, p in likelihoods]\n",
    "        return likelihoods\n",
    "    \n",
    "    def __call__(self, sentence: str, trial_count=5) -> List[float]:\n",
    "        results = {category[\"name\"]: 0.0 for category in self.categories}\n",
    "        for _ in range(trial_count):\n",
    "            likelihoods = self.classify_once(sentence)\n",
    "            for category, likelihood in likelihoods:\n",
    "                results[category] += likelihood / trial_count\n",
    "        results = list(results.items())\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: universalism\n",
      "Sentence: It is good to accept and try to understand those who are different from oneself.\n",
      "[('power', 1.27265606124638e-06), ('security', 8.745852113634099e-08), ('hedonism', 3.888645540099349e-12), ('universalism', 1.0556086657274182e-12), ('stimulation', 2.221720261672261e-13), ('self-direction', 1.0781985909243936e-16), ('conformity', 2.817910554125732e-17), ('achievement', 1.8855096633304625e-17), ('tradition', 6.323611187755675e-19), ('benevolence', 4.239685788541454e-25)]\n",
      "Incorrect!\n"
     ]
    }
   ],
   "source": [
    "classifier = LikelihoodBasedCategoryClassifier(categories)\n",
    "# take a random category \n",
    "category = categories[np.random.randint(len(categories))]\n",
    "sentence = np.random.choice(category[\"examples\"])\n",
    "print(f\"Category: {category['name']}\")\n",
    "print(f\"Sentence: {sentence}\")\n",
    "\n",
    "likelihoods = classifier(sentence)\n",
    "print(likelihoods)\n",
    "if likelihoods[0][0] == category[\"name\"]:\n",
    "    print(\"Correct!\")\n",
    "else:\n",
    "    print(\"Incorrect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "# basic evaluation go throguuh all categories and all examples\n",
    "def evaluate_classifier(classifier, categories, trial_count=5):\n",
    "    results = {category[\"name\"]: [] for category in categories}\n",
    "    \n",
    "    for category in categories:\n",
    "        for sentence in category[\"examples\"]:\n",
    "            likelihoods = classifier(sentence, trial_count=trial_count)\n",
    "            results[category[\"name\"]].append(likelihoods)\n",
    "    # compute accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for category in categories:\n",
    "        for likelihoods in results[category[\"name\"]]:\n",
    "            if likelihoods[0][0] == category[\"name\"]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total\n",
    "    return accuracy, results\n",
    "\n",
    "accuracy, results = evaluate_classifier(classifier, categories)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
