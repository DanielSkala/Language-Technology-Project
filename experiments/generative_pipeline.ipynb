{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mo\\Documents\\Organisations\\RUG\\Language-Technology-Project\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Union, Any, Optional\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), '../../Language-Technology-Project'))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    print(\"ALready in current dir\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device} is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_gb(model) = 5.80 GB\n",
      "vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model = AutoModelForCausalLM.from_pretrained('mosaicml/mpt-1b-redpajama-200b', trust_remote_code=True).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\", padding_side=\"left\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "print(f\"size_gb(model) = {model.num_parameters() * 4 / 1024**3:.2f} GB\")\n",
    "print(\"vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    }
   ],
   "source": [
    "def predict_next_token_logits(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    encoded_input['attention_mask'] = encoded_input['attention_mask'].bool()\n",
    "    output = model(**encoded_input)\n",
    "    next_token_logits = output.logits[:, -1, :]\n",
    "    return next_token_logits\n",
    "\n",
    "def decode_token_logits(logits):\n",
    "    return tokenizer.decode(torch.argmax(logits, dim=-1))\n",
    "\n",
    "text = \"Daniel Skala is\"\n",
    "logits = predict_next_token_logits(text)\n",
    "print(decode_token_logits(logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel skala is a member of the Swedish national team and has played for the Swedish national team since\n"
     ]
    }
   ],
   "source": [
    "text = \"Daniel skala is\"\n",
    "for i in range(16):\n",
    "    logits = predict_next_token_logits(text)\n",
    "    text += decode_token_logits(logits)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4', 0.20943237841129303), ('5', 0.17149624228477478), ('3', 0.14209160208702087), ('6', 0.06978660076856613), ('7', 0.05673491582274437), ('8', 0.031708039343357086), ('2', 0.027894824743270874), ('9', 0.022482799366116524), ('10', 0.007341871503740549)]\n",
      "[('a', 8.859507033776026e-06), ('an', 8.309674740303308e-06), ('one', 5.512757070391672e-06), ('the', 2.2303545392787782e-06)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_likelihoods_of_words_given_context(prompt: str, words: list, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of likelihoods of words given a prompt.\n",
    "    \"\"\"\n",
    "    logits = predict_next_token_logits(prompt)\n",
    "    likelihoods = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    word_probs = []\n",
    "    for word in words:\n",
    "        tokens = tokenizer.encode(word)\n",
    "        probability = 1\n",
    "        for token in tokens:\n",
    "            probability *= likelihoods[0, token].item()\n",
    "        # normalize probability so it doesnt penalize longer words\n",
    "        probability = probability ** (1/len(tokens))\n",
    "        word_probs.append((word, probability))\n",
    "    output = sorted(word_probs, key=lambda x: x[1], reverse=True)\n",
    "    output.sort(key=lambda x: x[1], reverse=True)\n",
    "    return output\n",
    "# example\n",
    "prompt = \"2+2=\"\n",
    "options = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "print(get_likelihoods_of_words_given_context(prompt, options, model, tokenizer))\n",
    "prompt = \"Daniel skala is \"\n",
    "options = [\"a\", \"the\", \"an\", \"one\"]\n",
    "print(get_likelihoods_of_words_given_context(prompt, options, model, tokenizer))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import categories\n",
    "\n",
    "class LikelihoodBasedCategoryClassifier:\n",
    "\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "\n",
    "    def __shuffle_categories(self):\n",
    "        np.random.shuffle(self.categories)\n",
    "    \n",
    "    def __construct_classification_prompt(self, sentence, max_length=512, categories=None):\n",
    "        prompt = \"Categories of Human Value Detection categories: \\n\\n\"\n",
    "        expectation2category = {}\n",
    "        for i, category in enumerate(categories):\n",
    "            name = category[\"name\"]\n",
    "            description = category[\"description\"]\n",
    "            description = description[:max_length] + \"...\" if len(description) > max_length else description\n",
    "            examples = category[\"examples\"]\n",
    "            example = examples[np.random.randint(len(examples))]\n",
    "            example = example[:max_length] + \"...\" if len(example) > max_length else example\n",
    "            option = chr(ord('1') + i)\n",
    "            prompt += f\"{option}. {name}: {description}\\n\"\n",
    "            expectation2category[option] = name\n",
    "        sentence = sentence[:max_length] + \"...\" if len(sentence) > max_length else sentence\n",
    "        prompt += f\"\\nargument: \\\"{sentence}\\\"\"\n",
    "        prompt += \"\\n\\nWhich category does the argument belong to? number \\n\"\n",
    "        return prompt, expectation2category\n",
    "\n",
    "    def classify_once(self, sentence: str) -> List[float]:\n",
    "        self.__shuffle_categories()\n",
    "        prompt, expectations = self.__construct_classification_prompt(sentence, categories=self.categories)\n",
    "        likelihoods = get_likelihoods_of_words_given_context(prompt, list(expectations.keys()), model, tokenizer)\n",
    "        likelihoods = [(expectations[c], p) for c, p in likelihoods]\n",
    "        return likelihoods\n",
    "    \n",
    "    def __call__(self, sentence: str, trial_count=3) -> List[float]:\n",
    "        results = {category[\"name\"]: 0.0 for category in self.categories}\n",
    "        for _ in range(trial_count):\n",
    "            likelihoods = self.classify_once(sentence)\n",
    "            for category, likelihood in likelihoods:\n",
    "                results[category] += likelihood / trial_count\n",
    "        results = list(results.items())\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: achievement\n",
      "Sentence: It is important to show your abilities.\n",
      "[('hedonism', 1.7475354923135455e-06), ('self-direction', 1.6712266320458487e-06), ('security', 1.0844106791788968e-06), ('stimulation', 1.0822098014765895e-06), ('benevolence', 8.768260689369829e-07), ('conformity', 5.353384911662336e-07), ('tradition', 5.213900872528635e-07), ('power', 3.934714195944859e-07), ('achievement', 3.339430255285455e-07), ('universalism', 2.4887009904735657e-07)]\n",
      "Incorrect!\n"
     ]
    }
   ],
   "source": [
    "classifier = LikelihoodBasedCategoryClassifier(categories)\n",
    "# take a random category \n",
    "category = categories[np.random.randint(len(categories))]\n",
    "sentence = np.random.choice(category[\"examples\"])\n",
    "print(f\"Category: {category['name']}\")\n",
    "print(f\"Sentence: {sentence}\")\n",
    "\n",
    "likelihoods = classifier(sentence)\n",
    "print(likelihoods)\n",
    "if likelihoods[0][0] == category[\"name\"]:\n",
    "    print(\"Correct!\")\n",
    "else:\n",
    "    print(\"Incorrect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# basic evaluation go throguuh all categories and all examples\n",
    "def evaluate_classifier(classifier, categories, trial_count=5):\n",
    "    results = {category[\"name\"]: [] for category in categories}\n",
    "    \n",
    "    for category in categories:\n",
    "        for sentence in category[\"examples\"]:\n",
    "            likelihoods = classifier(sentence, trial_count=trial_count)\n",
    "            results[category[\"name\"]].append(likelihoods)\n",
    "    # compute accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for category in categories:\n",
    "        for likelihoods in results[category[\"name\"]]:\n",
    "            if likelihoods[0][0] == category[\"name\"]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total\n",
    "    return accuracy, results\n",
    "\n",
    "accuracy, results = evaluate_classifier(classifier, categories)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
